{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, pickle, random\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "\n",
    "import qiskit\n",
    "from qiskit import QuantumCircuit, execute\n",
    "from qiskit.compiler import transpile\n",
    "from qiskit_aer import AerSimulator, QasmSimulator\n",
    "from qiskit.converters import circuit_to_dag, dag_to_circuit\n",
    "from qiskit.quantum_info import SparsePauliOp, Operator\n",
    "from qiskit.circuit.library import CXGate, RXGate, IGate, ZGate\n",
    "from qiskit.providers.fake_provider import FakeMontreal, FakeLima,FakeGuadalupe,FakeSherbrooke,FakePrague,FakeCairo\n",
    "from blackwater.data.utils import (\n",
    "    generate_random_pauli_sum_op,\n",
    "    create_estimator_meas_data,\n",
    "    circuit_to_graph_data_json,\n",
    "    get_backend_properties_v1,\n",
    "    encode_pauli_sum_op,\n",
    "    create_meas_data_from_estimators\n",
    ")\n",
    "from qiskit.circuit.random import random_circuit\n",
    "from tqdm import tqdm_notebook\n",
    "from mlp import MLP1, MLP2, MLP3, encode_data\n",
    "\n",
    "from mbd_utils import cal_z_exp, generate_disorder, construct_mbl_circuit, calc_imbalance, modify_and_add_noise_to_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from noise_utils import AddNoise, RemoveReadoutErrors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_random_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    print(f'random seed fixed to {seed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./multi_backend_model_data/tianyan_error.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "properties_dict = {'tianyan':data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将HLD_list追加保存到json文件\n",
    "def save_HLD(model_name,HLD_list):\n",
    "    # 定义两个字典\n",
    "    HLD_dict = {}\n",
    "    HLD_dict[model_name] = HLD_list\n",
    "    # 读取已有的 JSON 文件内容\n",
    "    with open('./multi_backend_model_data/HLD.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # 将第二个字典追加到已有内容中\n",
    "    data.update(HLD_dict)\n",
    "\n",
    "    # 将更新后的内容写回 JSON 文件\n",
    "    with open('./multi_backend_model_data/HLD.json', 'w') as file:\n",
    "        json.dump(data, file)\n",
    "\n",
    "\n",
    "# 将2个字典互补\n",
    "def complement_dicts(dict1, dict2):\n",
    "    # 找到两个字典的并集键集合\n",
    "    all_keys = set(dict1.keys()) | set(dict2.keys())\n",
    "\n",
    "    # 在每个字典中补足缺失的键值对\n",
    "    for key in all_keys:\n",
    "        dict1.setdefault(key, 0)\n",
    "        dict2.setdefault(key, 0)\n",
    "\n",
    "    return dict1, dict2\n",
    "\n",
    "\n",
    "def chunk_list(input_list, chunk_size):\n",
    "    return [input_list[i:i + chunk_size] for i in range(0, len(input_list), chunk_size)]\n",
    "\n",
    "# 加载训练和测试数据文件\n",
    "def load_circuits(data_files, f_ext='.pk'):\n",
    "    backend_name_list = []\n",
    "    circuits = []\n",
    "    ideal_exp_vals = []\n",
    "    noisy_exp_vals_1 = []\n",
    "    noisy_exp_vals_2 = []\n",
    "    noisy_exp_vals_3 = []\n",
    "    for data_file in data_files:\n",
    "        if f_ext == '.json':\n",
    "            for entry in json.load(open(data_file, 'r')):\n",
    "                # 获取当前线路对应的后端，这个后端名称可以从数据文件的名字获得\n",
    "                data_backend_name = os.path.basename(data_file).split('.')[0].split('_')[0]\n",
    "                backend_name_list.append(data_backend_name)\n",
    "                circuits.append(QuantumCircuit.from_qasm_str(entry['circuit']))\n",
    "                ideal_exp_vals.append(entry['ideal_exp_value'])\n",
    "                noisy_exp_vals_1.append(entry['noisy_exp_values_1'])\n",
    "                noisy_exp_vals_2.append(entry['noisy_exp_values_2'])\n",
    "                noisy_exp_vals_3.append(entry['noisy_exp_values_3'])\n",
    "        elif f_ext == '.pk':\n",
    "            for entry in pickle.load(open(data_file, 'rb')):\n",
    "                # 获取当前线路对应的后端，这个后端名称可以从数据文件的名字获得\n",
    "                data_backend_name = os.path.basename(data_file).split('.')[0].split('_')[0]\n",
    "                backend_name_list.append(data_backend_name)\n",
    "                circuits.append(entry['circuit'])\n",
    "                ideal_exp_vals.append(entry['ideal_exp_value'])\n",
    "                noisy_exp_vals_1.append(entry['noisy_exp_values_1'])\n",
    "                noisy_exp_vals_2.append(entry['noisy_exp_values_2'])\n",
    "                noisy_exp_vals_3.append(entry['noisy_exp_values_3'])\n",
    "    return circuits, ideal_exp_vals, noisy_exp_vals_1,noisy_exp_vals_2,noisy_exp_vals_3,backend_name_list\n",
    "\n",
    "\n",
    "# 获取数组中前25%大的数据和第25%-50%大的数据\n",
    "def get_25_50(arr):\n",
    "    arr.sort()  # 对数组进行排序\n",
    "    quarter = len(arr) // 4  # 计算数组长度的四分之一\n",
    "    first_25_percent = arr[-quarter:]  # 获取数组中前25%大的数据\n",
    "    second_25_to_50_percent = arr[-2*quarter:-quarter]  # 获取数组中第25%-50%大的数据\n",
    "    return first_25_percent, second_25_to_50_percent\n",
    "\n",
    "# 将一个形状如tensor([0.2600, 0.0900])的数据按照第一个为state，第二个位qubits转换为对应的二进制数\n",
    "def get_state(obs_tensor):\n",
    "    state = int(obs_tensor[0] * 100)\n",
    "    qubit = int(obs_tensor[1] * 100)\n",
    "    state_binary = format(state, '0' + str(qubit) + 'b')\n",
    "    return state_binary\n",
    "\n",
    "# 海灵格距离\n",
    "def HellingerDistance(p, q):\n",
    "    import numpy as np\n",
    "    p = np.array(p)\n",
    "    q = np.array(q)\n",
    "    return  np.sqrt(np.sum((np.sqrt(p) - np.sqrt(q))**2)) / np.sqrt(2)\n",
    "\n",
    "# JS散度\n",
    "def JensenShannonDivergence(p, q):\n",
    "    p = np.array(p)\n",
    "    q = np.array(q)\n",
    "    M = (p + q)/2\n",
    "    return 0.5 * np.sum(p*np.log(p/M)) + 0.5 * np.sum(q*np.log(q/M))\n",
    "\n",
    "# MSE\n",
    "def get_MSE(p,q):\n",
    "    p = np.array(p)\n",
    "    q = np.array(q)\n",
    "    mse = np.mean((p - q) ** 2)\n",
    "    return mse    \n",
    "\n",
    "# 获取某一数组的前百分之25元素的索引返回，如果不足1则返回最大元素的索引\n",
    "def get_top25_percent_indices(arr):\n",
    "    n = len(arr)\n",
    "    k = max(int(0.25 * n), 1)  # 获取前25%的数量，确保至少返回一个索引\n",
    "    if k >= n:  # 如果前25%的数量大于等于数组长度\n",
    "        return [np.argmax(arr)]  # 返回最大元素的索引\n",
    "    else:\n",
    "        indices = np.argpartition(arr, -k)[-k:]  # 获取前25%值对应的索引\n",
    "        return indices\n",
    "\n",
    "# 获取某一数组的前百分之50元素的索引返回，如果不足1则返回最大元素的索引\n",
    "def get_top50_percent_indices(arr):\n",
    "    n = len(arr)\n",
    "    k = max(int(0.5 * n), 1)  # 获取前50%的数量，确保至少返回一个索引\n",
    "    if k >= n:  # 如果前25%的数量大于等于数组长度\n",
    "        return [np.argmax(arr)]  # 返回最大元素的索引\n",
    "    else:\n",
    "        indices = np.argpartition(arr, -k)[-k:]  # 获取前25%值对应的索引\n",
    "        return indices\n",
    "\n",
    "\n",
    "# 获取数组中值不为0的元素的索引\n",
    "def get_not_0_index(arr):\n",
    "    indices = [i for i, value in enumerate(arr) if value != 0]\n",
    "    return indices\n",
    "\n",
    "# 获取线路中单比特和2比特门的个数,线路深度和宽度\n",
    "def get_circuit_info(trans_circuit):\n",
    "    dag = circuit_to_dag(trans_circuit)\n",
    "    Num_1Q_Gates = 0\n",
    "    Num_2Q_Gates = 0\n",
    "    circuit_depth = trans_circuit.depth()\n",
    "    circuit_weith = trans_circuit.width()\n",
    "    circuit_info = []\n",
    "    for node in dag.nodes():\n",
    "            try:\n",
    "                if node.qargs:\n",
    "                    if node.name==\"barrier\" or node.name==\"measure\":\n",
    "                        continue\n",
    "                    if len(node.qargs)==1:\n",
    "                        Num_1Q_Gates+=1\n",
    "                    else:\n",
    "                        Num_2Q_Gates+=1\n",
    "            except:\n",
    "                pass\n",
    "    # circuit_info.append(Num_1Q_Gates)\n",
    "    # circuit_info.append(Num_2Q_Gates)\n",
    "    circuit_info.append(circuit_depth)\n",
    "    circuit_info.append(circuit_weith)\n",
    "    return circuit_info\n",
    "\n",
    "# 获取num在arr中从小到大排序之后的位次\n",
    "def find_sorted_position(arr, num):\n",
    "    sorted_arr = sorted(arr)\n",
    "    return sorted_arr.index(num) + 1\n",
    "\n",
    "\n",
    "# 求得字典的key的合集之后根据新的key的集合更新字典并返回\n",
    "def update_noisy_dict(dict_1,dict_2,dict_3):\n",
    "    # 求取所有字典的key的合集\n",
    "    dicts = [dict_1,dict_2,dict_3]\n",
    "    keys = set().union(*dicts)\n",
    "    \n",
    "    # 更新每个字典\n",
    "    for d in dicts:\n",
    "        for key in keys:\n",
    "            if key not in d:\n",
    "                d[key] = 0\n",
    "    dict_1 = dicts[0]\n",
    "    dict_2 = dicts[1]\n",
    "    dict_3 = dicts[2]\n",
    "    return dict_1,dict_2,dict_3\n",
    "\n",
    "# 获取一个量子线路的门集统计信息\n",
    "def get_gate_info(qc):\n",
    "    gate_info = {}  # 用于存储门的信息\n",
    "    qubits_used = set()  # 用于存储使用过的比特索引的集合\n",
    "    two_qubits_gate = ['cx', 'cz', 'cy']\n",
    "    for instruction in qc._data:\n",
    "        op_name = instruction.operation.name\n",
    "        if op_name not in gate_info:\n",
    "            gate_info[op_name] = {}  # 初始化门的信息字典\n",
    "        qubits = instruction.qubits\n",
    "        if op_name in ['measure', 'barrier']:\n",
    "            continue  # 跳过测量和 barrier 操作\n",
    "        for qubit in qubits:\n",
    "            qubits_used.add(qubit.index)  # 将比特索引添加到集合中\n",
    "        if op_name in two_qubits_gate:\n",
    "            control, target = qubits\n",
    "            pair = (control.index, target.index)  # 控制-目标比特对\n",
    "            if pair not in gate_info[op_name]:\n",
    "                gate_info[op_name][pair] = 1  # 若该控制-目标比特对尚未记录，则初始化为1\n",
    "            else:\n",
    "                gate_info[op_name][pair] += 1  # 若已记录过该控制-目标比特对，则次数加1\n",
    "        else:\n",
    "            qubit_index = qubits[0].index  # 获取操作作用的比特索引\n",
    "            if qubit_index not in gate_info[op_name]:\n",
    "                gate_info[op_name][qubit_index] = 1  # 若该比特索引尚未记录，则初始化为1\n",
    "            else:\n",
    "                gate_info[op_name][qubit_index] += 1  # 若已记录过该比特索引，则次数加1\n",
    "    qubits_used = sorted(list(qubits_used))  # 将集合转换为排序后的列表\n",
    "    gate_info['qubits_num'] = qubits_used  # 添加比特数组到返回的字典中\n",
    "    return gate_info\n",
    "\n",
    "# 计算一个量子线路的error_info\n",
    "def get_error_info(properties, gate_info):\n",
    "    error_info = {}  # 存储门的错误信息\n",
    "    # 初始化\n",
    "    for gate in properties['gates_set']:\n",
    "        error_info[gate] = 0\n",
    "    for gate in gate_info:\n",
    "        if gate in properties['gates_set']:\n",
    "            error_sum = 0\n",
    "            for qubit, count in gate_info[gate].items():\n",
    "                if isinstance(qubit, tuple):  # 对于两比特门\n",
    "                    control, target = qubit\n",
    "                    gate_name = gate + f'_{control}_{target}'  # 按照惯例拼接门的名称\n",
    "                    gate_error = properties['gate_props'][gate_name]['gate_error']  # 获取门的错误率\n",
    "                    error_sum += count * gate_error\n",
    "                else:  # 对于单比特门\n",
    "                    gate_name = gate + f'_{qubit}'\n",
    "                    gate_error = properties['gate_props'][gate_name]['gate_error']  # 获取门的错误率\n",
    "                    error_sum += count * gate_error\n",
    "            error_info[gate] = error_sum\n",
    "    \n",
    "    # 计算使用过的qubits的readout_error的和\n",
    "    readout_error_sum = sum(properties['qubits_props'][qubit]['readout_error'] for qubit in gate_info['qubits_num'])\n",
    "\n",
    "    error_info['readout_error'] = readout_error_sum\n",
    "    error_info.pop('id')\n",
    "    error_info.pop('reset')\n",
    "    error_info.pop('rz')\n",
    "    \n",
    "    \n",
    "    return error_info\n",
    "\n",
    "\n",
    "# 获取天衍的error_info\n",
    "def get_tianyan_error(properties,gate_info):\n",
    "    total_statistic = 0.0\n",
    "    try:\n",
    "        rz = gate_info['rz']\n",
    "        for rz_gate,gate_num in rz.items():\n",
    "            total_statistic +=properties['singleQubit_error'][str(rz_gate)]*gate_num\n",
    "            # print(properties['singleQubit_error'][str(rz_gate)])\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        sx = gate_info['sx']\n",
    "        for sx_gate,gate_num in sx.items():\n",
    "            total_statistic +=properties['singleQubit_error'][str(sx_gate)]*gate_num\n",
    "            # print(properties['singleQubit_error'][str(sx_gate)])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        x = gate_info['x']\n",
    "        for x_gate,gate_num in x.items():\n",
    "            total_statistic +=properties['singleQubit_error'][str(x_gate)]*2*gate_num\n",
    "            # print(properties['singleQubit_error'][str(x_gate)])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        cx = gate_info['cx']\n",
    "        for cx_gate,gate_num in cx.items():\n",
    "            total_statistic +=properties['CZ_error'][str(cx_gate)]*gate_num\n",
    "            total_statistic +=properties['singleQubit_error'][[str(cx_gate[1])]]*2*gate_num\n",
    "            # print(properties['CZ_error'][str(cx_gate)])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        qubits = gate_info['qubits_num']\n",
    "        for qubit in qubits:\n",
    "            total_statistic +=properties['singleQubit_readoutError'][str(qubit)]\n",
    "            # print(properties['singleQubit_readoutError'][str(qubit)])\n",
    "    except:\n",
    "        pass\n",
    "        # print(total_statistic)\n",
    "    return total_statistic*0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qiskit.circuit.random\n",
    "import torch, random\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import json, os, pickle\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from qiskit import QuantumCircuit\n",
    "from scipy import stats\n",
    "\n",
    "def binary_to_decimal(binary_string):\n",
    "    decimal_number = int(binary_string, 2)\n",
    "    return decimal_number\n",
    "\n",
    "def count_lenth(list):\n",
    "    total_elements = 0\n",
    "    for dict_i in list:\n",
    "        total_elements += len(dict_i)\n",
    "    return total_elements\n",
    "\n",
    "def count_gates_by_rotation_angle(circuit, bin_size):\n",
    "    angles = []\n",
    "    for instr, qargs, cargs in circuit.data:\n",
    "        if instr.name in ['rx', 'ry', 'rz'] and len(qargs) == 1:\n",
    "            angles += [float(instr.params[0])]\n",
    "    bin_edges = np.arange(-2 * np.pi, 2 * np.pi + bin_size, bin_size)\n",
    "    counts, _ = np.histogram(angles, bins=bin_edges)\n",
    "    bin_labels = [f\"{left:.2f} to {right:.2f}\" for left, right in zip(bin_edges[:-1], bin_edges[1:])]\n",
    "    angle_bins = {label: count for label, count in zip(bin_labels, counts)}\n",
    "    return list(angle_bins.values())\n",
    "\n",
    "\n",
    "def recursive_dict_loop(my_dict, parent_key=None, out=None, target_key1=None, target_key2=None):\n",
    "    if out is None: out = []\n",
    "\n",
    "    for key, val in my_dict.items():\n",
    "        if isinstance(val, dict):\n",
    "            recursive_dict_loop(val, key, out, target_key1, target_key2)\n",
    "        else:\n",
    "            if parent_key and target_key1 in str(parent_key) and key == target_key2:\n",
    "                out += [val]\n",
    "    return out or 0.\n",
    "\n",
    "def encode_data(circuits,backend_name_list,properties_dict, ideal_vals_list, noisy_vals_list_1,noisy_vals_list_2,noisy_vals_list_3):\n",
    "    # circuits是量子线路，properties是后端属性，ideal_exp_vals和noisy_exp_vals分别是理想和含噪声的状态解(其数据类型是数组，数组元素是字典，每一个字典是一条线路的状态解)\n",
    "\n",
    "    # gates_set = sorted(['reset', 'rz', 'sx', 'x', 'id', 'cx'])     # must sort!\n",
    "    # noisy_gates_set = sorted([ 'sx', 'x',  'cx', 'rz'])\n",
    "    gates_set = sorted([ 'sx', 'x', 'cx','rz'])\n",
    "    # backend_code = {'fake_guadalupe':1,'fake_montreal':2,'fake_cairo':3,'fake_mumbai':4,'fake_sydney':5,'fake_toronto':6}\n",
    "    # FakeGuadalupe,FakeMontreal,FakeCairo,FakeMumbai,FakeSydney,FakeToronto\n",
    "\n",
    "    # vec = [np.mean(recursive_dict_loop(properties, out=[], target_key1='cx', target_key2='gate_error'))]\n",
    "    # vec += [np.mean(recursive_dict_loop(properties, out=[], target_key1='id', target_key2='gate_error'))]\n",
    "    # vec += [np.mean(recursive_dict_loop(properties, out=[], target_key1='sx', target_key2='gate_error'))]\n",
    "    # vec += [np.mean(recursive_dict_loop(properties, out=[], target_key1='x', target_key2='gate_error'))]\n",
    "    # vec += [np.mean(recursive_dict_loop(properties, out=[], target_key1='rz', target_key2='gate_error'))]\n",
    "    # vec += [np.mean(recursive_dict_loop(properties, out=[], target_key1='', target_key2='readout_error'))]\n",
    "    # vec += [np.mean(recursive_dict_loop(properties, out=[], target_key1='', target_key2='t1'))]\n",
    "    # vec += [np.mean(recursive_dict_loop(properties, out=[], target_key1='', target_key2='t2'))]\n",
    "    # vec = torch.tensor(vec) * 100  # put it in the same order of magnitude as the expectation values\n",
    "\n",
    "\n",
    "\n",
    "    bin_size = 0.5 * np.pi\n",
    "    num_angle_bins = int(np.ceil(4 * np.pi / bin_size))\n",
    "\n",
    "    # 首先更新含噪声的3个数组字典\n",
    "    for i,circ in enumerate(circuits):\n",
    "        noisy_vals_list_1[i],noisy_vals_list_2[i],noisy_vals_list_3[i] = update_noisy_dict(noisy_vals_list_1[i],noisy_vals_list_2[i],noisy_vals_list_3[i])\n",
    "    # 为每一个含噪声的状态解都要创建一行数据\n",
    "    X_lenth = count_lenth(noisy_vals_list_1)\n",
    "\n",
    "    X = torch.zeros([X_lenth, 1  + len(gates_set) + num_angle_bins +2 + 3 + 1 + 2])\n",
    "\n",
    "    error_slice = slice(0, 1)\n",
    "    # backend_code_slice = slice(1,1+1)\n",
    "    gate_counts_slice = slice(1, 1+len(gates_set))\n",
    "    angle_bins_slice = slice(1+len(gates_set), 1+len(gates_set)+num_angle_bins)\n",
    "    # exp_val_slice = slice(len(vec)+len(gates_set)+num_angle_bins, len(vec)+len(gates_set)+num_angle_bins+num_qubits)\n",
    "    # 线路状态信息\n",
    "    gate_info_slice = slice(1+len(gates_set)+num_angle_bins,1+len(gates_set)+num_angle_bins+2)\n",
    "    # 当前状态的概率值\n",
    "    exp_val_slice = slice(1+len(gates_set)+num_angle_bins+2, 1+len(gates_set)+num_angle_bins+2+3)\n",
    "    # 当前状态的概率值的波动程度，标准差\n",
    "    std_slice = slice(1+len(gates_set)+num_angle_bins+2+3,1+len(gates_set)+num_angle_bins+2+3+1)\n",
    "    # 二进制的状态编码转换为一个十进制数和对应的量子比特数\n",
    "    state_slice = slice(1+len(gates_set)+num_angle_bins+2+3+1,1+len(gates_set)+num_angle_bins+2+3+1+2)\n",
    "    # meas_basis_slice = slice(len(vec)+len(gates_set)+num_angle_bins+1+2, len(X[0]))\n",
    "    # 当前线路对应的运行后端的编码\n",
    "\n",
    "    x_count = 0\n",
    "    ideal_vals = []\n",
    "    for i, circ in enumerate(circuits):\n",
    "        gate_counts_all = circ.count_ops()\n",
    "        # 同一条线路的这个数据是相等的，所以这里统一赋值\n",
    "        backend_name = backend_name_list[i]\n",
    "        # backend_code_i = backend_code[backend_name]\n",
    "        properties = properties_dict[backend_name]\n",
    "        circuit_info = get_circuit_info(circ)\n",
    "        gate_counts = count_gates_by_rotation_angle(circ, bin_size)\n",
    "        noisy_vals_1 = noisy_vals_list_1[i]\n",
    "        noisy_vals_2 = noisy_vals_list_2[i]\n",
    "        noisy_vals_3 = noisy_vals_list_3[i]\n",
    "        gate_info = get_gate_info(circ)\n",
    "        error_info = get_tianyan_error(properties,gate_info)\n",
    "        # error_info = sum(np.array(list(error_info.values())))\n",
    "        for noisy_state in noisy_vals_1.keys():\n",
    "            obs = []\n",
    "            state = binary_to_decimal(noisy_state)*0.01\n",
    "            qubits = len(noisy_state)*0.01\n",
    "            obs.append(state)\n",
    "            obs.append(qubits)\n",
    "            noisy_vals = [noisy_vals_1[noisy_state],noisy_vals_2[noisy_state],noisy_vals_3[noisy_state]]\n",
    "            std_noisy =  stats.tstd(noisy_vals)\n",
    "            # 当前后端的编号\n",
    "            # X[x_count,backend_code_slice] = torch.tensor(backend_code_i)\n",
    "            # 3次含噪声的值的标准差\n",
    "            X[x_count,std_slice] = torch.tensor(std_noisy)\n",
    "            # 门错误统计信息\n",
    "            X[x_count,error_slice] = torch.tensor(error_info)\n",
    "            # 线路信息编码，4维\n",
    "            X[x_count,gate_info_slice] = torch.tensor(circuit_info)*0.01\n",
    "            # 状态编码，二维\n",
    "            X[x_count, state_slice] = torch.tensor(obs, dtype=torch.float)\n",
    "            # 对应状态解的编码，3维,因为线路在含噪声的情况下运行了3次\n",
    "            X[x_count, exp_val_slice] = torch.tensor(noisy_vals,dtype=torch.float)\n",
    "            # 将对应的理想解按X的噪声解顺序加到y中\n",
    "            if noisy_state in ideal_vals_list[i].keys():\n",
    "                ideal_vals.append(ideal_vals_list[i][noisy_state])\n",
    "            else:\n",
    "                ideal_vals.append(0)\n",
    "            X[x_count, gate_counts_slice] = torch.tensor([gate_counts_all.get(key, 0) for key in gates_set]) * 0.01  # put it in the same order of magnitude as the expectation values\n",
    "            X[x_count, angle_bins_slice] = torch.tensor(gate_counts) * 0.01  # put it in the same order of magnitude as the expectation values\n",
    "            x_count += 1\n",
    "    \n",
    "    y = torch.tensor(ideal_vals, dtype=torch.float32)\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def get_test_loaders(test_circuits,backend_name_list, test_ideal_vals_list, test_noisy_vals_list_1,test_noisy_vals_list_2,test_noisy_vals_list_3,properties_dict,BATCH_SIZE = 32):\n",
    "    # 这个函数为每一条线路都生成一个test_loader，这样在测试的时候可以把同一条线路的结果放在一起进行处理\n",
    "    Test_loaders = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for i,single_circuit in enumerate(test_circuits):\n",
    "        single_circuit = [single_circuit]\n",
    "        single_ideal = [test_ideal_vals_list[i]]\n",
    "        single_noisy_1 = [test_noisy_vals_list_1[i]]\n",
    "        single_noisy_2 = [test_noisy_vals_list_2[i]]\n",
    "        single_noisy_3 = [test_noisy_vals_list_3[i]]\n",
    "        backend_name = [backend_name_list[i]]\n",
    "        # *******************\n",
    "        # x_test_i,y_test_i = encode_data(single_circuit,properties,single_ideal,single_noisy_1,single_noisy_2,single_noisy_3)\n",
    "        x_test_i, y_test_i = encode_data(single_circuit,backend_name,properties_dict,single_ideal,single_noisy_1,single_noisy_2,single_noisy_3)\n",
    "        test_dataset_i = TensorDataset(torch.Tensor(x_test_i), torch.Tensor(y_test_i))\n",
    "        test_loader_i = DataLoader(test_dataset_i, batch_size=BATCH_SIZE*1000, shuffle=False)\n",
    "        x_test_i = pd.DataFrame(x_test_i)\n",
    "        y_test_i = pd.DataFrame(y_test_i)\n",
    "        Test_loaders.append(test_loader_i)\n",
    "        X_test.append(x_test_i)\n",
    "        y_test.append(y_test_i)\n",
    "    return Test_loaders,X_test,y_test\n",
    "            \n",
    "def get_test_data(test_paths):\n",
    "    # 参数就是一个路径的数组\n",
    "    # test_paths = glob.glob(f\"{test_path}/dataset_**.pk\")\n",
    "    test_circuits, test_ideal_vals_list, test_noisy_vals_list_1,test_noisy_vals_list_2,test_noisy_vals_list_3,backend_name_list = load_circuits(test_paths,f_ext = '.pk')\n",
    "    shots = sum(test_ideal_vals_list[0].values())\n",
    "    test_ideal_vals_list = [{k: v / shots for k, v in d.items()} for d in test_ideal_vals_list]\n",
    "    # test_noisy_vals_list_1 = [{k: v / shots for k, v in d.items()} for d in test_noisy_vals_list_1]\n",
    "    # test_noisy_vals_list_2 = [{k: v / shots for k, v in d.items()} for d in test_noisy_vals_list_2]\n",
    "    # test_noisy_vals_list_3 = [{k: v / shots for k, v in d.items()} for d in test_noisy_vals_list_3]\n",
    "    test_loader_list,X_test_list, y_test_list = get_test_loaders(test_circuits, backend_name_list,test_ideal_vals_list, test_noisy_vals_list_1,test_noisy_vals_list_2,test_noisy_vals_list_3,properties_dict)\n",
    "    return test_loader_list,X_test_list, y_test_list\n",
    "\n",
    "def get_train_data(train_path):\n",
    "    BATCH_SIZE = 32\n",
    "    train_paths = glob.glob(f\"{train_path}/**.pk\")\n",
    "    train_circuits, train_ideal_vals_list, train_noisy_vals_list_1,train_noisy_vals_list_2,train_noisy_vals_list_3,backend_name_list = load_circuits(train_paths,f_ext = '.pk')\n",
    "    # 转换为概率值\n",
    "    shots = sum(train_ideal_vals_list[0].values())\n",
    "    train_ideal_vals_list = [{k: v / shots for k, v in d.items()} for d in train_ideal_vals_list]\n",
    "    # train_noisy_vals_list_1 = [{k: v / shots for k, v in d.items()} for d in train_noisy_vals_list_1]\n",
    "    # train_noisy_vals_list_2 = [{k: v / shots for k, v in d.items()} for d in train_noisy_vals_list_2]\n",
    "    # train_noisy_vals_list_3 = [{k: v / shots for k, v in d.items()} for d in train_noisy_vals_list_3]\n",
    "    ######################################\n",
    "    X_train, y_train = encode_data(train_circuits,backend_name_list,properties_dict, train_ideal_vals_list, train_noisy_vals_list_1,train_noisy_vals_list_2,train_noisy_vals_list_3)\n",
    "    train_dataset = TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    X_train = pd.DataFrame(X_train)#.iloc[:, -4:]\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    return train_loader,X_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './multi_backend_model_data/tianyan_train_data'\n",
    "train_loader,X_train,y_train = get_train_data(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './test_tianyan.csv'\n",
    "X_train.to_csv(save_path,  index=False,  mode='w',  header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['./multi_backend_model_data/tianyan_big_test_data\\\\tianyan_25_qubit.pk']]\n"
     ]
    }
   ],
   "source": [
    "# 按比特数加载数据\n",
    "# 按比特数加载测试数据\n",
    "test_path = './multi_backend_model_data/tianyan_big_test_data'\n",
    "test_paths = glob.glob(f\"{test_path}/**.pk\")\n",
    "# 将文件列表按照\n",
    "test_paths = sorted(test_paths,key=lambda x: int(x.split('_')[-2]))\n",
    "# print(test_paths)\n",
    "# 1种后端\n",
    "test_paths_split_by_qubits = chunk_list(test_paths,1)\n",
    "print(test_paths_split_by_qubits)\n",
    "# print(test_paths_split_by_qubits)\n",
    "test_loader_lists = []\n",
    "X_test_lists = []\n",
    "y_test_lists = []\n",
    "# suanfa_name = []\n",
    "for qubits_paths in test_paths_split_by_qubits:\n",
    "    # print(qubits_paths)\n",
    "    # 获取算法名字\n",
    "    # base_name =  os.path.basename(paths)\n",
    "    # suanfa_name.append(os.path.splitext(base_name)[0])\n",
    "    test_loader_list,X_test_list, y_test_list = get_test_data(qubits_paths)\n",
    "    test_loader_lists.append(test_loader_list)\n",
    "    X_test_lists.append(X_test_list)\n",
    "    y_test_lists.append(y_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "def save_model(rfr,model_name):\n",
    "    save_path = f\"./result\"\n",
    "    model_path = f\"{save_path}/{model_name}\"\n",
    "    result_path = f\"{model_path}/result\"\n",
    "    loss_path = model_path\n",
    "    if not os.path.exists(model_path):\n",
    "        os.mkdir(model_path)\n",
    "    if not os.path.exists(result_path):\n",
    "        os.mkdir(result_path)\n",
    "    with open(f\"{model_path}/{model_name}.pk\", 'wb') as f:\n",
    "        pickle.dump(rfr, f)\n",
    "# 加载模型\n",
    "def loade_model(model_name):\n",
    "    save_path = f\"./result\"\n",
    "    model_path = f\"{save_path}/{model_name}\"\n",
    "    result_path = f\"{model_path}/result\"\n",
    "    with open(f\"{model_path}/{model_name}.pk\", 'rb') as f:\n",
    "        rfr = pickle.load(f)\n",
    "    return rfr,result_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seed fixed to 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\30806\\.conda\\envs\\test\\lib\\site-packages\\sklearn\\base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(n_estimators=300)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(n_estimators=300)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(n_estimators=300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "fix_random_seed(0)\n",
    "rfr = RandomForestRegressor(n_estimators=300)\n",
    "rfr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0     1     2     3     4    5    6     7     8     9   ...   11  \\\n",
      "0      1.3979  0.08  0.13  0.09  0.07  0.0  0.0  0.03  0.05  0.02  ...  0.0   \n",
      "1      1.3979  0.08  0.13  0.09  0.07  0.0  0.0  0.03  0.05  0.02  ...  0.0   \n",
      "2      1.3979  0.08  0.13  0.09  0.07  0.0  0.0  0.03  0.05  0.02  ...  0.0   \n",
      "3      1.3979  0.08  0.13  0.09  0.07  0.0  0.0  0.03  0.05  0.02  ...  0.0   \n",
      "4      1.3979  0.08  0.13  0.09  0.07  0.0  0.0  0.03  0.05  0.02  ...  0.0   \n",
      "...       ...   ...   ...   ...   ...  ...  ...   ...   ...   ...  ...  ...   \n",
      "47729  1.3979  0.08  0.13  0.09  0.07  0.0  0.0  0.03  0.05  0.02  ...  0.0   \n",
      "47730  1.3979  0.08  0.13  0.09  0.07  0.0  0.0  0.03  0.05  0.02  ...  0.0   \n",
      "47731  1.3979  0.08  0.13  0.09  0.07  0.0  0.0  0.03  0.05  0.02  ...  0.0   \n",
      "47732  1.3979  0.08  0.13  0.09  0.07  0.0  0.0  0.03  0.05  0.02  ...  0.0   \n",
      "47733  1.3979  0.08  0.13  0.09  0.07  0.0  0.0  0.03  0.05  0.02  ...  0.0   \n",
      "\n",
      "        12    13    14       15       16       17        18             19  \\\n",
      "0      0.0  0.05  0.91  0.00037  0.00038  0.00029  0.000049      96.820000   \n",
      "1      0.0  0.05  0.91  0.00040  0.00045  0.00039  0.000032    2725.199951   \n",
      "2      0.0  0.05  0.91  0.00254  0.00234  0.00253  0.000113    3230.239990   \n",
      "3      0.0  0.05  0.91  0.00242  0.00311  0.00249  0.000380    2899.439941   \n",
      "4      0.0  0.05  0.91  0.00266  0.00262  0.00253  0.000067    3053.040039   \n",
      "...    ...   ...   ...      ...      ...      ...       ...            ...   \n",
      "47729  0.0  0.05  0.91  0.00000  0.00000  0.00001  0.000006    3475.360107   \n",
      "47730  0.0  0.05  0.91  0.00000  0.00000  0.00001  0.000006    8378.400391   \n",
      "47731  0.0  0.05  0.91  0.00000  0.00000  0.00001  0.000006   24437.400391   \n",
      "47732  0.0  0.05  0.91  0.00000  0.00000  0.00001  0.000006   11298.759766   \n",
      "47733  0.0  0.05  0.91  0.00000  0.00001  0.00000  0.000006  171492.640625   \n",
      "\n",
      "         20  \n",
      "0      0.25  \n",
      "1      0.25  \n",
      "2      0.25  \n",
      "3      0.25  \n",
      "4      0.25  \n",
      "...     ...  \n",
      "47729  0.25  \n",
      "47730  0.25  \n",
      "47731  0.25  \n",
      "47732  0.25  \n",
      "47733  0.25  \n",
      "\n",
      "[47734 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_test_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.1390e-01, 1.0000e-02, 2.0000e-02, 1.0000e-02, 2.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e-02, 1.0000e-02, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 4.0000e-02, 7.1000e-01, 3.7880e-01, 3.7360e-01, 3.8070e-01,\n",
      "         3.6756e-03, 1.9000e-01, 5.0000e-02],\n",
      "        [2.1390e-01, 1.0000e-02, 2.0000e-02, 1.0000e-02, 2.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e-02, 1.0000e-02, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 4.0000e-02, 7.1000e-01, 4.1310e-01, 4.1370e-01, 4.1870e-01,\n",
      "         3.0746e-03, 3.0000e-02, 5.0000e-02],\n",
      "        [2.1390e-01, 1.0000e-02, 2.0000e-02, 1.0000e-02, 2.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e-02, 1.0000e-02, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 4.0000e-02, 7.1000e-01, 3.0100e-02, 2.9700e-02, 3.1100e-02,\n",
      "         7.2111e-04, 1.1000e-01, 5.0000e-02],\n",
      "        [2.1390e-01, 1.0000e-02, 2.0000e-02, 1.0000e-02, 2.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e-02, 1.0000e-02, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 4.0000e-02, 7.1000e-01, 2.3600e-02, 2.7300e-02, 2.4100e-02,\n",
      "         2.0075e-03, 1.0000e-02, 5.0000e-02],\n",
      "        [2.1390e-01, 1.0000e-02, 2.0000e-02, 1.0000e-02, 2.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e-02, 1.0000e-02, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 4.0000e-02, 7.1000e-01, 2.3200e-02, 2.3200e-02, 2.0700e-02,\n",
      "         1.4434e-03, 1.8000e-01, 5.0000e-02],\n",
      "        [2.1390e-01, 1.0000e-02, 2.0000e-02, 1.0000e-02, 2.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e-02, 1.0000e-02, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 4.0000e-02, 7.1000e-01, 2.9200e-02, 2.6400e-02, 2.7400e-02,\n",
      "         1.4189e-03, 2.7000e-01, 5.0000e-02],\n",
      "        [2.1390e-01, 1.0000e-02, 2.0000e-02, 1.0000e-02, 2.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e-02, 1.0000e-02, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 4.0000e-02, 7.1000e-01, 1.3300e-02, 1.5100e-02, 1.2900e-02,\n",
      "         1.1719e-03, 1.0000e-01, 5.0000e-02],\n",
      "        [2.1390e-01, 1.0000e-02, 2.0000e-02, 1.0000e-02, 2.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e-02, 1.0000e-02, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 4.0000e-02, 7.1000e-01, 7.4000e-03, 8.0000e-03, 7.7000e-03,\n",
      "         3.0000e-04, 2.3000e-01, 5.0000e-02],\n",
      "        [2.1390e-01, 1.0000e-02, 2.0000e-02, 1.0000e-02, 2.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e-02, 1.0000e-02, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 4.0000e-02, 7.1000e-01, 4.4000e-03, 3.1000e-03, 3.5000e-03,\n",
      "         6.6583e-04, 9.0000e-02, 5.0000e-02],\n",
      "        [2.1390e-01, 1.0000e-02, 2.0000e-02, 1.0000e-02, 2.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e-02, 1.0000e-02, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 4.0000e-02, 7.1000e-01, 1.9800e-02, 2.1600e-02, 1.9800e-02,\n",
      "         1.0392e-03, 1.7000e-01, 5.0000e-02]])\n",
      "[0.48897366 0.48951634 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "for test_X,test_y in test_loader_list[0]:\n",
    "    print(test_X[:10])\n",
    "    with open(f\"./result/tianyan_model/tianyan_model.pk\", 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    out = model.predict(test_X[:10])\n",
    "    print(out)\n",
    "    # print(out)\n",
    "    # state_list = []\n",
    "    # ideal_list = []\n",
    "    # noisy_list = []\n",
    "    # mitigated_list = []\n",
    "    # result = {}\n",
    "    # out = model.predict(test_X[:, :])\n",
    "    \n",
    "    # for obs, ideal, noisy, mitigated in zip(\n",
    "    #     test_X[:,-2:],\n",
    "    #     test_y.tolist(),\n",
    "    #     test_X[:, -6:-3].tolist(),\n",
    "    #     out.tolist()\n",
    "    # ):\n",
    "    #     state_list.append(get_state(obs))\n",
    "    #     ideal_list.append(ideal)\n",
    "    #     noisy_list.append(np.mean(np.array(noisy)))\n",
    "    #     mitigated_list.append(mitigated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3979e+00, 8.0000e-02, 1.3000e-01, 9.0000e-02, 7.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 3.0000e-02, 5.0000e-02, 2.0000e-02, 3.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 5.0000e-02, 9.1000e-01, 3.7000e-04, 3.8000e-04, 2.9000e-04,\n",
      "         4.9329e-05, 9.6820e+01, 2.5000e-01],\n",
      "        [1.3979e+00, 8.0000e-02, 1.3000e-01, 9.0000e-02, 7.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 3.0000e-02, 5.0000e-02, 2.0000e-02, 3.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 5.0000e-02, 9.1000e-01, 4.0000e-04, 4.5000e-04, 3.9000e-04,\n",
      "         3.2146e-05, 2.7252e+03, 2.5000e-01],\n",
      "        [1.3979e+00, 8.0000e-02, 1.3000e-01, 9.0000e-02, 7.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 3.0000e-02, 5.0000e-02, 2.0000e-02, 3.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 5.0000e-02, 9.1000e-01, 2.5400e-03, 2.3400e-03, 2.5300e-03,\n",
      "         1.1269e-04, 3.2302e+03, 2.5000e-01],\n",
      "        [1.3979e+00, 8.0000e-02, 1.3000e-01, 9.0000e-02, 7.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 3.0000e-02, 5.0000e-02, 2.0000e-02, 3.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 5.0000e-02, 9.1000e-01, 2.4200e-03, 3.1100e-03, 2.4900e-03,\n",
      "         3.7978e-04, 2.8994e+03, 2.5000e-01],\n",
      "        [1.3979e+00, 8.0000e-02, 1.3000e-01, 9.0000e-02, 7.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 3.0000e-02, 5.0000e-02, 2.0000e-02, 3.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 5.0000e-02, 9.1000e-01, 2.6600e-03, 2.6200e-03, 2.5300e-03,\n",
      "         6.6583e-05, 3.0530e+03, 2.5000e-01],\n",
      "        [1.3979e+00, 8.0000e-02, 1.3000e-01, 9.0000e-02, 7.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 3.0000e-02, 5.0000e-02, 2.0000e-02, 3.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 5.0000e-02, 9.1000e-01, 3.0200e-03, 2.9000e-03, 3.1400e-03,\n",
      "         1.2000e-04, 3.0658e+03, 2.5000e-01],\n",
      "        [1.3979e+00, 8.0000e-02, 1.3000e-01, 9.0000e-02, 7.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 3.0000e-02, 5.0000e-02, 2.0000e-02, 3.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 5.0000e-02, 9.1000e-01, 2.9300e-03, 2.3600e-03, 2.5500e-03,\n",
      "         2.9023e-04, 2.9020e+03, 2.5000e-01],\n",
      "        [1.3979e+00, 8.0000e-02, 1.3000e-01, 9.0000e-02, 7.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 3.0000e-02, 5.0000e-02, 2.0000e-02, 3.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 5.0000e-02, 9.1000e-01, 2.5200e-03, 2.1500e-03, 2.4200e-03,\n",
      "         1.9140e-04, 2.8687e+03, 2.5000e-01],\n",
      "        [1.3979e+00, 8.0000e-02, 1.3000e-01, 9.0000e-02, 7.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 3.0000e-02, 5.0000e-02, 2.0000e-02, 3.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 5.0000e-02, 9.1000e-01, 3.3400e-03, 3.2100e-03, 3.1100e-03,\n",
      "         1.1533e-04, 2.8790e+03, 2.5000e-01],\n",
      "        [1.3979e+00, 8.0000e-02, 1.3000e-01, 9.0000e-02, 7.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 3.0000e-02, 5.0000e-02, 2.0000e-02, 3.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 5.0000e-02, 9.1000e-01, 3.5000e-04, 2.6000e-04, 2.3000e-04,\n",
      "         6.2450e-05, 3.0355e+03, 2.5000e-01]])\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "tensor([[1.3979e+00, 8.0000e-02, 1.3000e-01, 9.0000e-02, 7.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 3.0000e-02, 5.0000e-02, 2.0000e-02, 3.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 5.0000e-02, 9.1000e-01, 0.0000e+00, 1.0000e-05, 0.0000e+00,\n",
      "         5.7735e-06, 1.5063e+04, 2.5000e-01],\n",
      "        [1.3979e+00, 8.0000e-02, 1.3000e-01, 9.0000e-02, 7.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 3.0000e-02, 5.0000e-02, 2.0000e-02, 3.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 5.0000e-02, 9.1000e-01, 0.0000e+00, 1.0000e-05, 0.0000e+00,\n",
      "         5.7735e-06, 1.7856e+05, 2.5000e-01],\n",
      "        [1.3979e+00, 8.0000e-02, 1.3000e-01, 9.0000e-02, 7.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 3.0000e-02, 5.0000e-02, 2.0000e-02, 3.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 5.0000e-02, 9.1000e-01, 0.0000e+00, 0.0000e+00, 1.0000e-05,\n",
      "         5.7735e-06, 8.7667e+04, 2.5000e-01],\n",
      "        [1.3979e+00, 8.0000e-02, 1.3000e-01, 9.0000e-02, 7.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 3.0000e-02, 5.0000e-02, 2.0000e-02, 3.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 5.0000e-02, 9.1000e-01, 0.0000e+00, 1.0000e-05, 0.0000e+00,\n",
      "         5.7735e-06, 1.3890e+04, 2.5000e-01],\n",
      "        [1.3979e+00, 8.0000e-02, 1.3000e-01, 9.0000e-02, 7.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 3.0000e-02, 5.0000e-02, 2.0000e-02, 3.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 5.0000e-02, 9.1000e-01, 0.0000e+00, 0.0000e+00, 1.0000e-05,\n",
      "         5.7735e-06, 4.2397e+02, 2.5000e-01],\n",
      "        [1.3979e+00, 8.0000e-02, 1.3000e-01, 9.0000e-02, 7.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 3.0000e-02, 5.0000e-02, 2.0000e-02, 3.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 5.0000e-02, 9.1000e-01, 0.0000e+00, 0.0000e+00, 1.0000e-05,\n",
      "         5.7735e-06, 1.2841e+03, 2.5000e-01],\n",
      "        [1.3979e+00, 8.0000e-02, 1.3000e-01, 9.0000e-02, 7.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 3.0000e-02, 5.0000e-02, 2.0000e-02, 3.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 5.0000e-02, 9.1000e-01, 0.0000e+00, 0.0000e+00, 1.0000e-05,\n",
      "         5.7735e-06, 3.0595e+03, 2.5000e-01],\n",
      "        [1.3979e+00, 8.0000e-02, 1.3000e-01, 9.0000e-02, 7.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 3.0000e-02, 5.0000e-02, 2.0000e-02, 3.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 5.0000e-02, 9.1000e-01, 0.0000e+00, 0.0000e+00, 1.0000e-05,\n",
      "         5.7735e-06, 1.8100e+05, 2.5000e-01],\n",
      "        [1.3979e+00, 8.0000e-02, 1.3000e-01, 9.0000e-02, 7.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 3.0000e-02, 5.0000e-02, 2.0000e-02, 3.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 5.0000e-02, 9.1000e-01, 0.0000e+00, 0.0000e+00, 1.0000e-05,\n",
      "         5.7735e-06, 1.2963e+05, 2.5000e-01],\n",
      "        [1.3979e+00, 8.0000e-02, 1.3000e-01, 9.0000e-02, 7.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 3.0000e-02, 5.0000e-02, 2.0000e-02, 3.0000e-02, 0.0000e+00,\n",
      "         0.0000e+00, 5.0000e-02, 9.1000e-01, 0.0000e+00, 1.0000e-05, 0.0000e+00,\n",
      "         5.7735e-06, 3.2124e+03, 2.5000e-01]])\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for test_X,test_y in test_loader_list[0]:\n",
    "    print(test_X[:10])\n",
    "    with open(f\"./result/tianyan_model/tianyan_model.pk\", 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    out = model.predict(test_X[:10])\n",
    "    print(out)\n",
    "    # print(out)\n",
    "    # state_list = []\n",
    "    # ideal_list = []\n",
    "    # noisy_list = []\n",
    "    # mitigated_list = []\n",
    "    # result = {}\n",
    "    # out = model.predict(test_X[:, :])\n",
    "    \n",
    "    # for obs, ideal, noisy, mitigated in zip(\n",
    "    #     test_X[:,-2:],\n",
    "    #     test_y.tolist(),\n",
    "    #     test_X[:, -6:-3].tolist(),\n",
    "    #     out.tolist()\n",
    "    # ):\n",
    "    #     state_list.append(get_state(obs))\n",
    "    #     ideal_list.append(ideal)\n",
    "    #     noisy_list.append(np.mean(np.array(noisy)))\n",
    "    #     mitigated_list.append(mitigated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'tianyan_model'\n",
    "save_model(rfr,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_mited_test(model_name,test_loader_lists,save:bool):\n",
    "    HLD_list = []\n",
    "    count_right_list = []\n",
    "    model,result_path = loade_model(model_name)\n",
    "    for index_i_s,test_loader_list in enumerate(test_loader_lists):\n",
    "        qubits = index_i_s + 2\n",
    "        HLD_mited_this_qubits = []\n",
    "        # HLD_noisy_this_qubits = []\n",
    "        count_right = 0\n",
    "        count_all = 0\n",
    "        for index_i,test_loader in enumerate(test_loader_list):\n",
    "            for test_X,test_y in test_loader:\n",
    "                state_list = []\n",
    "                ideal_list = []\n",
    "                noisy_list = []\n",
    "                mitigated_list = []\n",
    "                result = {}\n",
    "                out = model.predict(test_X[:, :])\n",
    "                # print(type(out))\n",
    "                for obs, ideal, noisy, mitigated in zip(\n",
    "                    test_X[:,-2:],\n",
    "                    test_y.tolist(),\n",
    "                    test_X[:, -6:-3].tolist(),\n",
    "                    out.tolist()\n",
    "                ):\n",
    "                    state_list.append(get_state(obs))\n",
    "                    ideal_list.append(ideal)\n",
    "                    noisy_list.append(np.mean(np.array(noisy)))\n",
    "                    mitigated_list.append(mitigated)\n",
    "                    \n",
    "                mitigated_list = np.array(mitigated_list)\n",
    "                # 先使得最小值为小于0的为0，再归一化\n",
    "                mitigated_list = np.clip(mitigated_list, a_min=0, a_max=None)\n",
    "                mitigated_list = mitigated_list/sum(mitigated_list)\n",
    "                for ideal,noisy,mitigated in zip(ideal_list,noisy_list,mitigated_list):\n",
    "                    if(abs(ideal-mitigated)<abs(ideal-noisy)):\n",
    "                        count_right +=1\n",
    "                    count_all += 1\n",
    "                HLD_mited = HellingerDistance(ideal_list,mitigated_list)\n",
    "                print(HLD_mited)\n",
    "                # HLD_noisy = HellingerDistance(ideal_list,noisy_list)\n",
    "                HLD_mited_this_qubits.append(HLD_mited)\n",
    "                # HLD_noisy_this_qubits.append(HLD_noisy)\n",
    "                if (save):\n",
    "                    result['state'] = state_list\n",
    "                    result['ideal'] = ideal_list\n",
    "                    result['noisy'] = noisy_list\n",
    "                    result['mitigated'] = mitigated_list\n",
    "                    df = pd.DataFrame(result)\n",
    "                    file_name = f\"{result_path}/{qubits}_qubits_{index_i}.xlsx\"\n",
    "                    df.to_excel(file_name, index=False)\n",
    "                    print(f\"结果已保存到 {file_name} 文件中\")\n",
    "        mean_HLD_mited_this_qubits = np.mean(np.array(HLD_mited_this_qubits))\n",
    "        mitigated_right = count_right/count_all\n",
    "        HLD_list.append(mean_HLD_mited_this_qubits)\n",
    "        count_right_list.append(mitigated_right)\n",
    "    return HLD_list,count_right_list\n",
    "        # mean_HLD_noisy_this_qubits = np.mean(np.array(HLD_noisy_this_qubits))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_mited_test(model_name,test_loader_lists,save:bool):\n",
    "    HLD_list = []\n",
    "    count_right_list = []\n",
    "    model,result_path = loade_model(model_name)\n",
    "    for index_i_s,test_loader_list in enumerate(test_loader_lists):\n",
    "        qubits = index_i_s + 2\n",
    "        HLD_mited_this_qubits = []\n",
    "        # HLD_noisy_this_qubits = []\n",
    "        count_right = 0\n",
    "        count_all = 0\n",
    "        for index_i,test_loader in enumerate(test_loader_list):\n",
    "            for test_X,test_y in test_loader:\n",
    "                state_list = []\n",
    "                ideal_list = []\n",
    "                noisy_list = []\n",
    "                mitigated_list = []\n",
    "                result = {}\n",
    "                out = model.predict(test_X[:, :])\n",
    "                \n",
    "                for obs, ideal, noisy, mitigated in zip(\n",
    "                    test_X[:,-2:],\n",
    "                    test_y.tolist(),\n",
    "                    test_X[:, -6:-3].tolist(),\n",
    "                    out.tolist()\n",
    "                ):\n",
    "                    state_list.append(get_state(obs))\n",
    "                    ideal_list.append(ideal)\n",
    "                    noisy_list.append(np.mean(np.array(noisy)))\n",
    "                    mitigated_list.append(mitigated)\n",
    "                    \n",
    "                mitigated_list = np.array(mitigated_list)\n",
    "                # 先使得最小值为小于0的为0，再归一化\n",
    "                mitigated_list = np.clip(mitigated_list, a_min=0, a_max=None)\n",
    "                mitigated_list = mitigated_list/sum(mitigated_list)\n",
    "                for ideal,noisy,mitigated in zip(ideal_list,noisy_list,mitigated_list):\n",
    "                    if(abs(ideal-mitigated)<abs(ideal-noisy)):\n",
    "                        count_right +=1\n",
    "                    count_all += 1\n",
    "                HLD_mited = HellingerDistance(ideal_list,mitigated_list)\n",
    "                print(HLD_mited)\n",
    "                # HLD_noisy = HellingerDistance(ideal_list,noisy_list)\n",
    "                HLD_mited_this_qubits.append(HLD_mited)\n",
    "                # HLD_noisy_this_qubits.append(HLD_noisy)\n",
    "                if (save):\n",
    "                    result['state'] = state_list\n",
    "                    result['ideal'] = ideal_list\n",
    "                    result['noisy'] = noisy_list\n",
    "                    result['mitigated'] = mitigated_list\n",
    "                    df = pd.DataFrame(result)\n",
    "                    file_name = f\"{result_path}/{qubits}_qubits_{index_i}.xlsx\"\n",
    "                    df.to_excel(file_name, index=False)\n",
    "                    print(f\"结果已保存到 {file_name} 文件中\")\n",
    "        mean_HLD_mited_this_qubits = np.mean(np.array(HLD_mited_this_qubits))\n",
    "        mitigated_right = count_right/count_all\n",
    "        HLD_list.append(mean_HLD_mited_this_qubits)\n",
    "        count_right_list.append(mitigated_right)\n",
    "    return HLD_list,count_right_list\n",
    "        # mean_HLD_noisy_this_qubits = np.mean(np.array(HLD_noisy_this_qubits))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_mited_test_1(model_name,test_loader_lists,save:bool):\n",
    "    HLD_list = []\n",
    "    count_right_list = []\n",
    "    model,result_path = loade_model(model_name)\n",
    "    for index_i_s,test_loader_list in enumerate(test_loader_lists):\n",
    "        qubits = index_i_s + 2\n",
    "        HLD_mited_this_qubits = []\n",
    "        # HLD_noisy_this_qubits = []\n",
    "        count_right = 0\n",
    "        count_all = 0\n",
    "        for index_i,test_loader in enumerate(test_loader_list):\n",
    "            for test_X,test_y in test_loader:\n",
    "                state_list = []\n",
    "                ideal_list = []\n",
    "                noisy_list = []\n",
    "                mitigated_list = []\n",
    "                result = {}\n",
    "                out = model.predict(test_X[:, :])\n",
    "                \n",
    "                for obs, ideal, noisy, mitigated in zip(\n",
    "                    test_X[:,-2:],\n",
    "                    test_y.tolist(),\n",
    "                    test_X[:, -6:-3].tolist(),\n",
    "                    out.tolist()\n",
    "                ):\n",
    "                    state_list.append(get_state(obs))\n",
    "                    ideal_list.append(ideal)\n",
    "                    noisy_list.append(np.mean(np.array(noisy)))\n",
    "                    mitigated_list.append(mitigated)\n",
    "                    \n",
    "                mitigated_list = np.array(mitigated_list)\n",
    "                # 先使得最小值为小于0的为0，再归一化\n",
    "                mitigated_list = np.clip(mitigated_list, a_min=0, a_max=None)\n",
    "                mitigated_list = mitigated_list/sum(mitigated_list)\n",
    "                for ideal,noisy,mitigated in zip(ideal_list,noisy_list,mitigated_list):\n",
    "                    if(abs(ideal-mitigated)<abs(ideal-noisy)):\n",
    "                        count_right +=1\n",
    "                    count_all += 1\n",
    "                HLD_mited = HellingerDistance(ideal_list,mitigated_list)\n",
    "                print(HLD_mited)\n",
    "                # HLD_noisy = HellingerDistance(ideal_list,noisy_list)\n",
    "                HLD_mited_this_qubits.append(HLD_mited)\n",
    "                # HLD_noisy_this_qubits.append(HLD_noisy)\n",
    "                if (save):\n",
    "                    result['state'] = state_list\n",
    "                    result['ideal'] = ideal_list\n",
    "                    result['noisy'] = noisy_list\n",
    "                    result['mitigated'] = mitigated_list\n",
    "                    df = pd.DataFrame(result)\n",
    "                    file_name = f\"{result_path}/{qubits}_qubits_{index_i}.xlsx\"\n",
    "                    df.to_excel(file_name, index=False)\n",
    "                    print(f\"结果已保存到 {file_name} 文件中\")\n",
    "        mean_HLD_mited_this_qubits = np.mean(np.array(HLD_mited_this_qubits))\n",
    "        mitigated_right = count_right/count_all\n",
    "        HLD_list.append(mean_HLD_mited_this_qubits)\n",
    "        count_right_list.append(mitigated_right)\n",
    "    return HLD_list,count_right_list\n",
    "        # mean_HLD_noisy_this_qubits = np.mean(np.array(HLD_noisy_this_qubits))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_noisy_test(test_loader_lists):\n",
    "    HLD_list = []\n",
    "    # count_right_list = []\n",
    "    for index_i_s,test_loader_list in enumerate(test_loader_lists):\n",
    "        HLD_noisy_this_qubits = []\n",
    "        for index_i,test_loader in enumerate(test_loader_list):\n",
    "            for test_X,test_y in test_loader:\n",
    "                state_list = []\n",
    "                ideal_list = []\n",
    "                noisy_list = []\n",
    "                for obs, ideal, noisy in zip(\n",
    "                    test_X[:,-2:],\n",
    "                    test_y.tolist(),\n",
    "                    test_X[:, -6:-3].tolist()\n",
    "                ):\n",
    "                    state_list.append(get_state(obs))\n",
    "                    ideal_list.append(ideal)\n",
    "                    noisy_list.append(np.mean(np.array(noisy)))\n",
    "                HLD_noisy = HellingerDistance(ideal_list,noisy_list)\n",
    "                HLD_noisy_this_qubits.append(HLD_noisy)\n",
    "                print(HLD_noisy)\n",
    "        mean_HLD_noisy_this_qubits = np.mean(np.array(HLD_noisy_this_qubits))\n",
    "        HLD_list.append(mean_HLD_noisy_this_qubits)\n",
    "\n",
    "    return HLD_list\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6577530747306497\n",
      "0.7844169297347104\n",
      "0.6049069049168848\n",
      "0.5841259437811431\n",
      "0.6781111785855851\n",
      "[0.6618628063497946]\n"
     ]
    }
   ],
   "source": [
    "noisy = get_model_noisy_test(test_loader_lists)\n",
    "print(noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(noisy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 假设 out_1 和 out_2 是两个 numpy 数组\n",
    "out_1 = np.array([1, 2, 3])\n",
    "out_2 = np.array([4, 5, 6])\n",
    "\n",
    "# 沿着指定的轴将两个数组拼接起来\n",
    "combined_array = np.concatenate((out_1, out_2), axis=0)\n",
    "\n",
    "# 如果是二维数组，可以指定沿着行或列进行拼接\n",
    "# combined_array = np.concatenate((out_1, out_2), axis=1)\n",
    "\n",
    "print(combined_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35879320677422943\n",
      "1.0\n",
      "0.11466177797976965\n",
      "0.3908812625534154\n",
      "0.5323311419808815\n"
     ]
    }
   ],
   "source": [
    "tianyan_HLD_list,count_right_listHLD = get_model_mited_test('tianyan_model',test_loader_lists,save =False)\n",
    "# 注意，之后整理的时候，要把第二条线路去掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4793334778576591]\n"
     ]
    }
   ],
   "source": [
    "print(tianyan_HLD_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_HLD('tianyan_error',noisy)\n",
    "save_HLD('tianyan_mited',tianyan_HLD_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
